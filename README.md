# ğŸ“ˆ Ad Sales Forecasting â€” MLOps Time Series Project

> **Production-grade** MLOps pipeline for forecasting daily advertisement revenue using ARIMA, SARIMAX, and LSTM models, with full MLflow experiment tracking, model registry, and inference.

---

## ğŸ—‚ï¸ Project Structure

```
ad-sales-forecasting/
â”‚
â”œâ”€â”€ venv/                          â† Python virtual environment (created by you)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ global_ads_performance_dataset.csv
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ processed_ads.csv
â”‚       â”œâ”€â”€ feature_scaler.pkl
â”‚       â”œâ”€â”€ target_scaler.pkl
â”‚       â””â”€â”€ inference_predictions.csv
â”‚
â”œâ”€â”€ logs/                          â† âœ¨ Auto-created per-run log files
â”‚   â”œâ”€â”€ 20260225_133000_train.log  â† timestamped log (YYYYMMDD_HHMMSS_label)
â”‚   â”œâ”€â”€ 20260225_140000_infer.log
â”‚   â”œâ”€â”€ run_history.txt            â† manifest of every run ever launched
â”‚   â””â”€â”€ archive/                   â† gzip-compressed old logs (auto-managed)
â”‚       â””â”€â”€ 20260224_120000_train.log.gz
â”‚
â”œâ”€â”€ notebooks/                     â† Jupyter exploration notebooks (optional)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py             â† Load & aggregate daily time-series
â”‚   â”œâ”€â”€ preprocessing.py           â† Clean, scale, split (forward-chain)
â”‚   â”œâ”€â”€ feature_engineering.py     â† Lags, rolling windows, calendar features
â”‚   â”œâ”€â”€ logger_setup.py            â† âœ¨ Centralised file + console logging
â”‚   â”œâ”€â”€ evaluate.py                â† MAE / RMSE / MAPE / RÂ² + plots
â”‚   â”œâ”€â”€ train.py                   â† Master training orchestrator
â”‚   â”œâ”€â”€ inference.py               â† 7-day forecast from registered model
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ arima_model.py         â† ARIMA grid search + MLflow logging
â”‚       â”œâ”€â”€ sarimax_model.py       â† SARIMAX with exogenous features
â”‚       â””â”€â”€ lstm_model.py          â† PyTorch LSTM + early stopping + CUDA
â”‚
â”œâ”€â”€ artifacts/                     â† Local artifact store (auto-created)
â”‚   â”œâ”€â”€ arima/
â”‚   â”œâ”€â”€ sarimax/
â”‚   â””â”€â”€ lstm/
â”‚
â”œâ”€â”€ mlruns/                        â† MLflow local tracking store
â”‚
â”œâ”€â”€ config.yaml                    â† Central configuration file
â”œâ”€â”€ requirements.txt               â† Pinned dependencies
â”œâ”€â”€ main.py                        â† CLI entry-point
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment Setup

### 1. Create Virtual Environment

```bash
cd ad-sales-forecasting
python -m venv venv
```

### 2. Activate Virtual Environment

**Windows (PowerShell)**
```powershell
venv\Scripts\Activate.ps1
```

**Windows (CMD)**
```cmd
venv\Scripts\activate.bat
```

**Linux / macOS**
```bash
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

> ğŸ’¡ The project auto-detects CUDA availability.  
> If a GPU is available, LSTM training runs on GPU automatically.

---

## ï¿½ File Logging

Every run automatically creates a **timestamped log file** in the `logs/` folder.

### Log File Naming

```
logs/
  YYYYMMDD_HHMMSS_train.log   â† one file per training run
  YYYYMMDD_HHMMSS_infer.log   â† one file per inference run
  run_history.txt             â† master manifest of all runs
  archive/
    YYYYMMDD_HHMMSS_train.log.gz   â† auto-gzipped when limit exceeded
```

### How Archive Works

| Setting | Default | Description |
|---|---|---|
| `logging.max_log_files` | `5` | Max active `.log` files in `logs/` |
| Archive trigger | automatic | When the 6th run starts, the **oldest** log is gzip-compressed and moved to `logs/archive/` |
| Compression | gzip | Archived logs shrink ~80-90% compared to plain text |

### `run_history.txt` â€” Run Manifest

Every time a run starts, one line is appended:

```
2026-02-25 13:30:00  [     train]  â†’ 20260225_133000_train.log
2026-02-25 14:00:00  [     infer]  â†’ 20260225_140000_infer.log
```

This gives you a **permanent audit trail** of every run, even after logs are archived.

### Controlling Log Behaviour via `config.yaml`

```yaml
logging:
  log_dir:      "logs"   # folder where .log files are saved
  max_log_files: 5       # how many active logs before oldest is archived
  level:        "INFO"   # DEBUG | INFO | WARNING | ERROR
```

### Console vs File Output

| Handler | Format | Colours |
|---|---|---|
| Console (stdout) | `TIMESTAMP [LEVEL] module â€“ message` | âœ… ANSI coloured |
| File (`.log`) | `TIMESTAMP [LEVEL] module â€“ message` | âŒ plain text |

---



### Full Training (ARIMA + SARIMAX + LSTM)

```bash
python main.py train
```

### Train with Custom Config / Seed

```bash
python main.py train --config config.yaml --seed 123
```

### Inference Only (after training)

```bash
python main.py infer
```

### Forecast 14 Days

```bash
python main.py infer --days 14
```

### View Project Info

```bash
python main.py info
```

---

## ğŸ“Š MLflow Experiment Tracking

### Launch MLflow UI

```bash
mlflow ui --backend-store-uri mlruns
```

Then open your browser at: **http://127.0.0.1:5000**

### View Experiments

Navigate to the **"Ad_Sales_TimeSeries"** experiment to see:

| Level | Name Format | Example |
|---|---|---|
| Parent | `{MODEL}_main_experiment` | `ARIMA_main_experiment` |
| Child | `{MODEL}_p{p}_d{d}_q{q}` | `ARIMA_p1_d1_q1` |
| Child | `LSTM_lr{lr}_bs{bs}_seq{seq}` | `LSTM_lr0.001_bs32_seq14` |

### Metrics Logged (per child run)

| Metric | Description |
|---|---|
| `test_mae` | Mean Absolute Error |
| `test_rmse` | Root Mean Squared Error |
| `test_mape` | Mean Absolute Percentage Error |
| `test_r2` | RÂ² Coefficient of Determination |
| `training_time_sec` | Wall-clock training time |
| `epoch_train_loss` | Per-epoch training loss (LSTM) |
| `epoch_val_loss` | Per-epoch validation loss (LSTM) |

### Artifacts Logged (per child run)

```
model/
  â”œâ”€â”€ arima_model.pkl / sarimax_model.pkl / lstm_model.pt
  â””â”€â”€ model_summary.txt

plots/
  â”œâ”€â”€ {MODEL}_predictions.png
  â””â”€â”€ {MODEL}_residuals.png

feature_importance/
  â””â”€â”€ exog_coefficients.txt   (SARIMAX only)

inference/
  â””â”€â”€ inference_predictions.csv
```

---

## ğŸ—ï¸ Model Details

### A) ARIMA

- Grid search over `p âˆˆ {1,2}`, `d âˆˆ {1}`, `q âˆˆ {1,2}`
- Trained on unscaled raw target (handles stationarity internally)
- Uses `statsmodels.tsa.arima.model.ARIMA`

### B) SARIMAX

- Extends ARIMA with **seasonal components** and **exogenous ad features**:
  `impressions, clicks, CTR, CPC, ad_spend, conversions, CPA, ROAS`
- Seasonal order: `(P=1, D=1, Q=1, s=7)` â€” weekly seasonality
- Logs exog variable coefficients as feature importance

### C) LSTM (PyTorch)

| Component | Detail |
|---|---|
| Architecture | 2-layer stacked LSTM + linear head |
| Input | Sliding window of `seq_length` timesteps |
| Features | All exog cols + target (scaled) |
| CUDA | Auto-detected; logs `device_used` tag |
| Early stopping | Triggered if val loss stagnates (patience=7) |

---

## ğŸ—ƒï¸ Model Registry

After training, the **best model** (lowest RMSE) is:

1. Logged to its MLflow run artifact store
2. Registered as **`Ad_Sales_Forecaster`** in MLflow Model Registry
3. Transitioned to **Staging** stage automatically

View in MLflow UI â†’ **Models** tab

---

## ğŸ“‹ Configuration (`config.yaml`)

```yaml
data:
  train_ratio: 0.70   # 70% training data
  val_ratio:   0.15   # 15% validation
  test_ratio:  0.15   # 15% test

arima:
  param_grid:
    p: [1, 2]
    d: [1]
    q: [1, 2]

sarimax:
  param_grid:
    p: [1, 2]
    d: [1]
    q: [1, 2]
    seasonal_order: [[1, 1, 1, 7]]

lstm:
  param_grid:
    learning_rate: [0.001, 0.01]
    batch_size: [32, 64]
    sequence_length: [14, 21]
  hidden_size: 128
  num_layers: 2
  epochs: 50
  patience: 7
```

---

## ğŸ–¥ï¸ Sample Output

```
2026-02-25 13:00:00 [INFO] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2026-02-25 13:00:00 [INFO] STEP 1 â€” Loading raw data
2026-02-25 13:00:01 [INFO]   Daily rows: 365
2026-02-25 13:00:01 [INFO] STEP 2 â€” Feature engineering
2026-02-25 13:00:01 [INFO]   Feature engineering complete â†’ 32 features
2026-02-25 13:00:01 [INFO] STEP 4a â€” ARIMA Experiment
2026-02-25 13:00:02 [INFO]   ARIMA parent run started â€¦
2026-02-25 13:00:02 [INFO]   Fitting ARIMA_p1_d1_q1 â€¦
2026-02-25 13:00:05 [INFO]     ARIMA_p1_d1_q1 â†’ RMSE=1234.56 | RÂ²=0.8732
...
2026-02-25 13:05:00 [INFO] OVERALL BEST â†’ SARIMAX | SARIMAX_p1_d1_q1_P1_D1_Q1_s7
2026-02-25 13:05:01 [INFO]   RMSE  : 987.43
2026-02-25 13:05:01 [INFO]   MAE   : 654.21
2026-02-25 13:05:01 [INFO]   MAPE  : 8.34%
2026-02-25 13:05:01 [INFO]   RÂ²    : 0.9241
2026-02-25 13:05:02 [INFO]   Model v1 â†’ Staging âœ“

7-Day Revenue Forecast:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  2026-12-31 â†’   $ 12,345.67
  2027-01-01 â†’   $ 13,102.44
  ...
```

---

## ğŸ“¸ Screenshots

> **MLflow Experiments View**
> *(After running training, open http://127.0.0.1:5000)*

```
Experiment: Ad_Sales_TimeSeries
â”œâ”€â”€ ARIMA_main_experiment
â”‚   â”œâ”€â”€ ARIMA_p1_d1_q1  [RMSE: 1234.56]
â”‚   â”œâ”€â”€ ARIMA_p1_d1_q2  [RMSE: 1198.34]
â”‚   â”œâ”€â”€ ARIMA_p2_d1_q1  [RMSE: 1210.45]
â”‚   â””â”€â”€ ARIMA_p2_d1_q2  [RMSE: 1201.77]
â”œâ”€â”€ SARIMAX_main_experiment
â”‚   â”œâ”€â”€ SARIMAX_p1_d1_q1_P1_D1_Q1_s7  [RMSE: 987.43] â† BEST
â”‚   â””â”€â”€ ...
â””â”€â”€ LSTM_main_experiment
    â”œâ”€â”€ LSTM_lr0.001_bs32_seq14
    â””â”€â”€ ...

Model Registry: Ad_Sales_Forecaster v1 [Staging]
```

---

## ğŸ”¬ Dataset

| Column | Type | Description |
|---|---|---|
| `date` | Date | Daily date index |
| `impressions` | int | Ad impressions |
| `clicks` | int | Ad clicks |
| `CTR` | float | Click-through rate |
| `CPC` | float | Cost per click |
| `ad_spend` | float | Total ad spend ($) |
| `conversions` | int | Conversion count |
| `CPA` | float | Cost per acquisition |
| **`revenue`** | **float** | **Target: daily revenue ($)** |
| `ROAS` | float | Return on ad spend |

---

## ğŸ¤– CUDA Support

```python
# Automatic detection (lstm_model.py)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

- CPU: Compatible with any machine
- GPU: Significantly faster LSTM training
- `device_used` is logged as an MLflow tag per LSTM run

---

## ğŸ› ï¸ Troubleshooting

| Issue | Fix |
|---|---|
| `FileNotFoundError: dataset not found` | Ensure CSV is in `data/raw/` |
| `MLflow tracking URI error` | Run from project root directory |
| `ARIMA convergence warning` | Increase `maxiter` in `ARIMA.fit()` |
| `CUDA out of memory` | Reduce `batch_size` in config.yaml |
| `Port 5000 already in use` | Run `mlflow ui --port 5001` |

---

## ğŸ“¦ Key Dependencies

| Package | Purpose |
|---|---|
| `statsmodels` | ARIMA, SARIMAX models |
| `torch` | LSTM + GPU support |
| `mlflow` | Experiment tracking + registry |
| `pandas` | Time-series data manipulation |
| `scikit-learn` | Metrics + preprocessing |
| `matplotlib` | Visualization |
| `click` | CLI argument parsing |
| `pyyaml` | Config file parsing |

---

*Built with â¤ï¸ as a production-grade MLOps Time Series project.*
